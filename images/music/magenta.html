<html>
  <head>
    <!-- Load @magenta/music -->
    <script src="https://cdn.jsdelivr.net/npm/@magenta/music@1.0.0"></script>

    <!-- Place your code in the script tag below. You can also use an external .js file -->
    <script>
      // Instantiate the model by loading the desired checkpoint.
      const model = new mm.MusicVAE(
          'https://storage.googleapis.com/download.magenta.tensorflow.org/' +
          'tfjs_checkpoints/music_vae/trio_4bar_lokl_small_q1');
      const player = new mm.Player();

      const start = () => {
        document.getElementById("start").style.display = "none";
        // Resume AudioContext on user action to enable audio.
        mm.Player.tone.context.resume();
        model.initialize().then(
          // Endlessly sample and play back the result.
          function sampleAndPlay() {
            return model.sample(1)
                .then((samples) => player.start(samples[0]))
                .then(sampleAndPlay);
          })};
    </script>
  </head>
  <body>
    <button id="start" onclick="start()">Start To Play</button>
    <img src="diagram.png" width="600" />

<p>
When a painter creates a work of art, she first blends colors on an artistâ€™s palette before applying to the canvas. 

<p>
Through machine learning (Based on SketchRNN for latent space model), musicians and composers can create palettes for blending and exploring musical scores.

<p>
Click on the start button above to enjoy a music combined from two melodies.

<p>
Code based on Google tensorflow Magenta framework.

  </body>
</html>
